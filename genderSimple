!pip install transformers torch scikit-learn pandas
!unzip -o gender_based.zip -d gender_based
!unzip -o indonesia.zip    -d indonesia
!unzip -o santiment.zip    -d santiment

import numpy as np
import pandas as pd

SEED = 42
np.random.seed(SEED)

GBV_PATH = "gender_based/Train.csv"
gbv_df = pd.read_csv(GBV_PATH)
gbv_df = gbv_df.dropna(subset=["tweet", "type"]).reset_index(drop=True)

gbv_df["text"] = gbv_df["tweet"]
gbv_df["gbv_label"] = 1 

print("GBV –¥–∞—Ç–∞—Å–µ—Ç:", gbv_df.shape, gbv_df["type"].value_counts())


INDO_PATH = "indonesia/data.csv"
indo_df = pd.read_csv(INDO_PATH, encoding="ISO-8859-1")
indo_df = indo_df.dropna(subset=["Tweet"]).reset_index(drop=True)


mask_non_gender_abusive = (
    (indo_df["HS_Gender"] == 0)
    & ((indo_df["HS"] == 1) | (indo_df["Abusive"] == 1))
)
indo_neg = indo_df[mask_non_gender_abusive].copy()
indo_neg["text"] = indo_neg["Tweet"]
indo_neg["gbv_label"] = 0

print("Indonesian NON-GENDER abusive:", indo_neg.shape)

SENT_PATH = "santiment/twitter_training.csv"


sent_df = pd.read_csv(SENT_PATH, header=None)


sent_df.columns = ["id", "entity", "sentiment", "tweet"]

print(sent_df.head())


sent_df = sent_df.dropna(subset=["tweet", "sentiment"]).reset_index(drop=True)

mask_pos_neu = sent_df["sentiment"].isin(["Positive", "Neutral"])
sent_safe = sent_df[mask_pos_neu].copy()
sent_safe["text"] = sent_safe["tweet"]
sent_safe["gbv_label"] = 0

print("Sentiment POS/NEU:", sent_safe.shape, sent_safe["sentiment"].value_counts())


N_POS = 5000




import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import classification_report
from transformers import AutoTokenizer, AutoModel
from torch.optim import AdamW

SEED = 42
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("–ü—Ä–∏—Å—Ç—Ä—ñ–π:", device)


X = all_df["text"].tolist()
y = all_df["gbv_label"].astype(int).values

X_train, X_val, y_train, y_val = train_test_split(
    X, y,
    test_size=0.15,
    random_state=SEED,
    stratify=y
)

print("Train size:", len(X_train), "Val size:", len(X_val))
print("–†–æ–∑–ø–æ–¥—ñ–ª y_train:", np.bincount(y_train))


MODEL_NAME = "xlm-roberta-base"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
max_length = 128

class GBVDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        encoding = self.tokenizer(
            text,
            add_special_tokens=True,
            truncation=True,
            max_length=self.max_length,
            padding="max_length",
            return_tensors="pt"
        )
        item = {k: v.squeeze(0) for k, v in encoding.items()}
        item["labels"] = torch.tensor(self.labels[idx], dtype=torch.long)
        return item

train_dataset = GBVDataset(X_train, y_train, tokenizer, max_length)
val_dataset   = GBVDataset(X_val,   y_val,   tokenizer, max_length)

batch_size = 16
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False)


class GBVTransformer(nn.Module):
    """
    –ë—ñ–Ω–∞—Ä–Ω–∏–π –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä:
      –≤—Ö—ñ–¥  ‚Äî —Ç–≤—ñ—Ç,
      –≤–∏—Ö—ñ–¥ ‚Äî Non_GBV (0) / GBV (1).

    –û—Å–æ–±–ª–∏–≤—ñ—Å—Ç—å:
      –∑–∞–º—ñ—Å—Ç—å –æ–¥–Ω–æ–≥–æ CLS –∑ –æ—Å—Ç–∞–Ω–Ω—å–æ–≥–æ —à–∞—Ä—É
      –±–µ—Ä–µ–º–æ CLS –∑ –æ—Å—Ç–∞–Ω–Ω—ñ—Ö 4 —à–∞—Ä—ñ–≤ —ñ —É—Å–µ—Ä–µ–¥–Ω—é—î–º–æ —ó—Ö (layer-wise pooling).
    """
    def __init__(self, model_name: str, num_labels: int = 2, class_weights=None):
        super().__init__()
        self.encoder = AutoModel.from_pretrained(
            model_name,
            output_hidden_states=True
        )
        hidden_size = self.encoder.config.hidden_size
        self.dropout = nn.Dropout(0.3)
        self.classifier = nn.Linear(hidden_size, num_labels)

        if class_weights is not None:
            self.class_weights = torch.tensor(class_weights, dtype=torch.float)
        else:
            self.class_weights = None

    def forward(self, input_ids, attention_mask, labels=None):
        outputs = self.encoder(
            input_ids=input_ids,
            attention_mask=attention_mask
        )
        hidden_states = outputs.hidden_states

        cls_tokens = torch.stack(
            [
                hidden_states[-1][:, 0],
                hidden_states[-2][:, 0],
                hidden_states[-3][:, 0],
                hidden_states[-4][:, 0],
            ],
            dim=0
        )

        cls_pooled = torch.mean(cls_tokens, dim=0)  # (batch, hidden)

        x = self.dropout(cls_pooled)
        logits = self.classifier(x)

        loss = None
        if labels is not None:
            if self.class_weights is not None:
                cw = self.class_weights.to(labels.device)
                loss_fct = nn.CrossEntropyLoss(weight=cw)
            else:
                loss_fct = nn.CrossEntropyLoss()
            loss = loss_fct(logits, labels)

        return logits, loss


class_weights = compute_class_weight(
    class_weight="balanced",
    classes=np.array([0, 1]),
    y=y_train
)
print("–í–∞–≥–∏ –∫–ª–∞—Å—ñ–≤ (0=Non_GBV, 1=GBV):", class_weights)

model = GBVTransformer(
    model_name=MODEL_NAME,
    num_labels=2,
    class_weights=class_weights
).to(device)


epochs = 3        
optimizer = AdamW(model.parameters(), lr=2e-5)

def train_one_epoch(epoch_idx: int):
    model.train()
    total_loss = 0.0

    for batch in train_loader:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        optimizer.zero_grad()
        logits, loss = model(input_ids=input_ids,
                             attention_mask=attention_mask,
                             labels=labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    avg_loss = total_loss / len(train_loader)
    print(f"[–ï–ø–æ—Ö–∞ {epoch_idx+1}] train loss = {avg_loss:.4f}")

def evaluate():
    model.eval()
    all_labels, all_preds = [], []

    with torch.no_grad():
        for batch in val_loader:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)

            logits, _ = model(input_ids=input_ids,
                              attention_mask=attention_mask)
            preds = torch.argmax(logits, dim=1)

            all_labels.extend(labels.cpu().numpy())
            all_preds.extend(preds.cpu().numpy())

    print("–í–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω—ñ –º–µ—Ç—Ä–∏–∫–∏ (Non_GBV vs GBV):")
    print(classification_report(
        all_labels,
        all_preds,
        target_names=["Non_GBV", "GBV"],
        digits=3
    ))

for epoch in range(epochs):
    train_one_epoch(epoch)
    evaluate()


import os, json

output_dir = "gbv_xlmr_binary_5000_5000"
os.makedirs(output_dir, exist_ok=True)

torch.save(model.state_dict(), os.path.join(output_dir, "model.bin"))
tokenizer.save_pretrained(output_dir)

with open(os.path.join(output_dir, "label_mapping.json"), "w", encoding="utf-8") as f:
    json.dump({"0": "Non_GBV", "1": "GBV"}, f, ensure_ascii=False, indent=2)

print("–ú–æ–¥–µ–ª—å –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤:", output_dir)


def predict_gbv(texts, threshold: float = 0.5):
    """
    texts: —Å–ø–∏—Å–æ–∫ —Ä—è–¥–∫—ñ–≤ (—Ç–≤—ñ—Ç—ñ–≤)
    threshold: –ø–æ—Ä—ñ–≥ —É–ø–µ–≤–Ω–µ–Ω–æ—Å—Ç—ñ –¥–ª—è –±–ª–æ–∫—É–≤–∞–Ω–Ω—è

    –ü–æ–≤–µ—Ä—Ç–∞—î —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–Ω–∏–∫—ñ–≤:
      { "text", "label", "probability", "decision" }
    """
    model.eval()
    enc = tokenizer(
        texts,
        padding=True,
        truncation=True,
        max_length=max_length,
        return_tensors="pt"
    )
    enc = {k: v.to(device) for k, v in enc.items()}

    with torch.no_grad():
        logits, _ = model(
            input_ids=enc["input_ids"],
            attention_mask=enc["attention_mask"]
        )
        probs = torch.softmax(logits, dim=1)
        preds = torch.argmax(probs, dim=1)

    results = []
    for i, text in enumerate(texts):
        label_id = int(preds[i].cpu().numpy())
        prob = float(probs[i, label_id].cpu().numpy())
        label_name = "GBV" if label_id == 1 else "Non_GBV"
        decision = "BLOCK" if (label_id == 1 and prob >= threshold) else "ALLOW"
        results.append({
            "text": text,
            "label": label_name,
            "probability": prob,
            "decision": decision
        })
    return results

sample_texts = [
    "He kept beating her and humiliating her because she is a woman.",
    "I hate this government, they are so corrupt.",
    "Today is a beautiful sunny day!"
]

preds = predict_gbv(sample_texts, threshold=0.6)
for item in preds:
    print("="*80)
    print("–¢–µ–∫—Å—Ç:", item["text"])
    print(f"–ö–ª–∞—Å: {item['label']} (p={item['probability']:.3f})")
    print("–†—ñ—à–µ–Ω–Ω—è —Ñ—ñ–ª—å—Ç—Ä–∞:", item["decision"])
N_NEG = 5000

gbv_sample = gbv_df.sample(
    n=min(N_POS, len(gbv_df)),
    random_state=SEED
).copy()
gbv_sample["source"] = "GBV"

neg_pool = pd.concat(
    [indo_neg, sent_safe],
    ignore_index=True
)

neg_sample = neg_pool.sample(
    n=min(N_NEG, len(neg_pool)),
    random_state=SEED
).copy()
neg_sample["source"] = "Indo_or_Sentiment"

all_df = pd.concat(
    [
        gbv_sample[["text", "gbv_label", "source"]],
        neg_sample[["text", "gbv_label", "source"]],
    ],
    ignore_index=True
)

print("–§—ñ–Ω–∞–ª—å–Ω–∏–π –∑–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω–∏–π –¥–∞—Ç–∞—Å–µ—Ç:", all_df.shape)
print("–†–æ–∑–ø–æ–¥—ñ–ª –∫–ª–∞—Å—ñ–≤:\n", all_df["gbv_label"].value_counts())
print("–†–æ–∑–ø–æ–¥—ñ–ª –¥–∂–µ—Ä–µ–ª:\n", all_df["source"].value_counts())

import os
import json
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModel
import gradio as gr


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("–ü—Ä–∏—Å—Ç—Ä—ñ–π –¥–ª—è —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É:", device)

BASE_MODEL_NAME = "xlm-roberta-base"
MODEL_DIR = "gbv_xlmr_binary_5000_5000"
max_length = 128

class GBVTransformer(nn.Module):
    def __init__(self, model_name: str, num_labels: int = 2):
        super().__init__()
        self.encoder = AutoModel.from_pretrained(
            model_name,
            output_hidden_states=True
        )
        hidden_size = self.encoder.config.hidden_size
        self.dropout = nn.Dropout(0.3)
        self.classifier = nn.Linear(hidden_size, num_labels)

    def forward(self, input_ids, attention_mask):
        outputs = self.encoder(
            input_ids=input_ids,
            attention_mask=attention_mask
        )
        hidden_states = outputs.hidden_states

        cls_tokens = torch.stack(
            [
                hidden_states[-1][:, 0],
                hidden_states[-2][:, 0],
                hidden_states[-3][:, 0],
                hidden_states[-4][:, 0],
            ],
            dim=0
        )  

        cls_pooled = torch.mean(cls_tokens, dim=0)

        x = self.dropout(cls_pooled)
        logits = self.classifier(x)
        return logits

tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)

model = GBVTransformer(BASE_MODEL_NAME, num_labels=2).to(device)
state_dict = torch.load(os.path.join(MODEL_DIR, "model.bin"), map_location=device)
model.load_state_dict(state_dict)
model.eval()

id2label = {0: "Non_GBV", 1: "GBV"}
label2ua = {
    "Non_GBV": "–Ω–µ–º–∞—î –æ–∑–Ω–∞–∫ –≥–µ–Ω–¥–µ—Ä–Ω–æ –∑—É–º–æ–≤–ª–µ–Ω–æ–≥–æ –Ω–∞—Å–∏–ª—å—Å—Ç–≤–∞",
    "GBV": "–≤–∏—è–≤–ª–µ–Ω—ñ –æ–∑–Ω–∞–∫–∏ –≥–µ–Ω–¥–µ—Ä–Ω–æ –∑—É–º–æ–≤–ª–µ–Ω–æ–≥–æ –Ω–∞—Å–∏–ª—å—Å—Ç–≤–∞"
}

def _predict_proba(texts):
    """–ü–æ–≤–µ—Ä—Ç–∞—î –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ –∫–ª–∞—Å—ñ–≤ –¥–ª—è —Å–ø–∏—Å–∫—É —Ç–µ–∫—Å—Ç—ñ–≤."""
    if isinstance(texts, str):
        texts = [texts]

    enc = tokenizer(
        texts,
        padding=True,
        truncation=True,
        max_length=max_length,
        return_tensors="pt"
    )
    enc = {k: v.to(device) for k, v in enc.items()}

    with torch.no_grad():
        logits = model(
            input_ids=enc["input_ids"],
            attention_mask=enc["attention_mask"]
        )
        probs = torch.softmax(logits, dim=1).cpu().numpy() 

    return probs


def classify_single(text, threshold):
    """–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –æ–¥–Ω–æ–≥–æ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è."""
    text = (text or "").strip()
    if not text:
        return (
            "‚Äî",
            "‚Äî",
            "–í–≤–µ–¥—ñ—Ç—å —Ç–µ–∫—Å—Ç –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É.",
            pd.DataFrame(columns=["–ö–ª–∞—Å", "–ô–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å"])
        )

    probs = _predict_proba([text])[0]
    pred_id = int(np.argmax(probs))
    pred_label = id2label[pred_id]
    prob = float(probs[pred_id])

    decision = "BLOCK" if (pred_label == "GBV" and prob >= threshold) else "ALLOW"


    if pred_label == "GBV":
        summary = (
            f"üö´ **–ë–ª–æ–∫—É–≤–∞—Ç–∏**: –º–æ–¥–µ–ª—å –≤–∏—è–≤–∏–ª–∞ {label2ua[pred_label]} "
            f"(p = {prob:.3f}, –ø–æ—Ä—ñ–≥ = {threshold:.2f})."
        )
    else:
        summary = (
            f"‚úÖ **–î–æ–∑–≤–æ–ª–∏—Ç–∏**: –º–æ–¥–µ–ª—å –Ω–µ –≤–∏—è–≤–∏–ª–∞ –≤–ø–µ–≤–Ω–µ–Ω–∏—Ö –æ–∑–Ω–∞–∫ "
            f"–≥–µ–Ω–¥–µ—Ä–Ω–æ –∑—É–º–æ–≤–ª–µ–Ω–æ–≥–æ –Ω–∞—Å–∏–ª—å—Å—Ç–≤–∞ (p(GBV) = {probs[1]:.3f}, "
            f"–ø–æ—Ä—ñ–≥ = {threshold:.2f})."
        )

    df = pd.DataFrame(
        {
            "–ö–ª–∞—Å": ["Non_GBV", "GBV"],
            "–ô–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å": [probs[0], probs[1]]
        }
    )

    return pred_label, f"{prob:.3f}", summary, df


def classify_batch(text_block, file, threshold):
    """
    –ü–∞–∫–µ—Ç–Ω–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞:
      - –∞–±–æ –±–∞–≥–∞—Ç–æ—Ä—è–¥–∫–æ–≤–∏–π —Ç–µ–∫—Å—Ç (–ø–æ –æ–¥–Ω–æ–º—É –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—é –≤ —Ä—è–¥–∫—É),
      - –∞–±–æ —Ñ–∞–π–ª .txt / .csv (—É .csv –±–µ—Ä–µ–º–æ –ø–µ—Ä—à—É —Ç–µ–∫—Å—Ç–æ–≤—É –∫–æ–ª–æ–Ω–∫—É).
    """
    texts = []

    if file is not None:
        path = file.name
        ext = os.path.splitext(path)[1].lower()

        if ext == ".txt":
            with open(path, "r", encoding="utf-8", errors="ignore") as f:
                texts = [line.strip() for line in f.readlines() if line.strip()]
        elif ext == ".csv":
            df = pd.read_csv(path)
            candidate_cols = [c for c in df.columns if c.lower() in ["text", "tweet", "message"]]
            if candidate_cols:
                col = candidate_cols[0]
            else:
                col = df.columns[0]
            texts = [str(x) for x in df[col].tolist() if str(x).strip()]
        else:
            return "–ü—ñ–¥—Ç—Ä–∏–º—É—é—Ç—å—Å—è –ª–∏—à–µ .txt —Ç–∞ .csv —Ñ–∞–π–ª–∏.", None
    else:
        
        if text_block:
            texts = [line.strip() for line in text_block.split("\n") if line.strip()]

    if not texts:
        return "–ù–µ–º–∞—î –∂–æ–¥–Ω–æ–≥–æ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É.", None

    probs = _predict_proba(texts)
    preds = np.argmax(probs, axis=1)

    labels = [id2label[int(i)] for i in preds]
    probs_gbv = probs[:, 1]
    decisions = [
        "BLOCK" if (labels[i] == "GBV" and probs_gbv[i] >= threshold) else "ALLOW"
        for i in range(len(texts))
    ]

    res_df = pd.DataFrame(
        {
            "text": texts,
            "label": labels,
            "p_GBV": probs_gbv,
            "decision": decisions,
        }
    )

    out_path = "gbv_batch_results.csv"
    res_df.to_csv(out_path, index=False, encoding="utf-8")

    msg = (
        f"–û–±—Ä–æ–±–ª–µ–Ω–æ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å: {len(texts)}.\n"
        f"GBV (BLOCK): {(res_df['decision']=='BLOCK').sum()}, "
        f"Non_GBV (ALLOW): {(res_df['decision']=='ALLOW').sum()}."
    )

    return msg, res_df, out_path




custom_css = """
#app-title {
    text-align: center;
    font-size: 2.3rem;
    font-weight: 800;
    margin-bottom: 0.5rem;
}
#app-subtitle {
    text-align: center;
    font-size: 1.0rem;
    opacity: 0.85;
}
.gradio-container {
    max-width: 1100px !important;
    margin: 0 auto !important;
}
"""

with gr.Blocks(css=custom_css, title="GBV Shield") as demo:

    gr.Markdown("# Gender-based violence Shield ", elem_id="app-title")
    gr.Markdown(
        "–°–∏—Å—Ç–µ–º–∞ –Ω–µ–π—Ä–æ–º–µ—Ä–µ–∂–µ–≤–æ–≥–æ –≤–∏—è–≤–ª–µ–Ω–Ω—è —Ç–∞ —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å "
        "—ñ–∑ –æ–∑–Ω–∞–∫–∞–º–∏ –≥–µ–Ω–¥–µ—Ä–Ω–æ –∑—É–º–æ–≤–ª–µ–Ω–æ–≥–æ –Ω–∞—Å–∏–ª—å—Å—Ç–≤–∞ —É –≤–µ–±—Å–µ—Ä–µ–¥–æ–≤–∏—â—ñ.",
        elem_id="app-subtitle"
    )

    with gr.Tab("üì© –ê–Ω–∞–ª—ñ–∑ –æ–∫—Ä–µ–º–∏—Ö –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å"):
        with gr.Row():
            with gr.Column(scale=2):
                single_input = gr.Textbox(
                    label="–í–≤–µ–¥—ñ—Ç—å —Ç–µ–∫—Å—Ç –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è / —Ç–≤—ñ—Ç–∞",
                    lines=5,
                    placeholder="He kept beating her and humiliating her because she is a woman..."
                )
                threshold_single = gr.Slider(
                    minimum=0.3,
                    maximum=0.9,
                    value=0.6,
                    step=0.01,
                    label="–ü–æ—Ä—ñ–≥ –¥–ª—è —Ä—ñ—à–µ–Ω–Ω—è BLOCK (–∑–∞ –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—é GBV)"
                )
                btn_single = gr.Button("–ê–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏")

            with gr.Column(scale=1):
                pred_label = gr.Textbox(label="–ú–æ–¥–µ–ª—å–Ω–∏–π –∫–ª–∞—Å (Non_GBV / GBV)", interactive=False)
                pred_prob = gr.Textbox(label="–ô–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å –æ–±—Ä–∞–Ω–æ–≥–æ –∫–ª–∞—Å—É", interactive=False)
                summary_md = gr.Markdown(label="–ö–æ—Ä–æ—Ç–∫–∏–π –≤–∏—Å–Ω–æ–≤–æ–∫")
                probs_table = gr.DataFrame(
                    headers=["–ö–ª–∞—Å", "–ô–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å"],
                    label="–ô–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ –ø–æ –∫–ª–∞—Å–∞—Ö",
                    interactive=False
                )

        btn_single.click(
            fn=classify_single,
            inputs=[single_input, threshold_single],
            outputs=[pred_label, pred_prob, summary_md, probs_table]
        )

    with gr.Tab("üì¶ –ü–∞–∫–µ—Ç–Ω–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞"):
        with gr.Row():
            with gr.Column():
                batch_text = gr.Textbox(
                    label="–¢–µ–∫—Å—Ç–∏ (–ø–æ –æ–¥–Ω–æ–º—É –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—é –≤ —Ä—è–¥–∫—É)",
                    lines=10,
                    placeholder="Each line is a separate message...\nOne tweet per line..."
                )
                batch_file = gr.File(
                    label="–ê–ë–û –∑–∞–≤–∞–Ω—Ç–∞–∂—Ç–µ —Ñ–∞–π–ª (.txt –∞–±–æ .csv)",
                    file_types=[".txt", ".csv"],
                    type="filepath"
                )
                threshold_batch = gr.Slider(
                    minimum=0.3,
                    maximum=0.9,
                    value=0.6,
                    step=0.01,
                    label="–ü–æ—Ä—ñ–≥ –¥–ª—è —Ä—ñ—à–µ–Ω–Ω—è BLOCK (–∑–∞ –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—é GBV)"
                )
                btn_batch = gr.Button("–ó–∞–ø—É—Å—Ç–∏—Ç–∏ –ø–∞–∫–µ—Ç–Ω—É –ø–µ—Ä–µ–≤—ñ—Ä–∫—É")

            with gr.Column():
                batch_info = gr.Markdown(label="–ü—ñ–¥—Å—É–º–æ–∫")
                batch_table = gr.DataFrame(
                    label="–†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó",
                    interactive=False
                )
                batch_file_out = gr.File(
                    label="–ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ —É —Ñ–æ—Ä–º–∞—Ç—ñ CSV"
                )

        btn_batch.click(
            fn=classify_batch,
            inputs=[batch_text, batch_file, threshold_batch],
            outputs=[batch_info, batch_table, batch_file_out]
        )

    with gr.Tab("üìñ –î–æ–≤—ñ–¥–∫–∞ –ø—Ä–æ –º–æ–¥–µ–ª—å —Ç–∞ –¥–∞—Ç–∞—Å–µ—Ç–∏"):
        gr.Markdown(
            """
### –û–ø–∏—Å –º–æ–¥–µ–ª—ñ

- **–ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞:** –±–∞–≥–∞—Ç–æ–º–æ–≤–Ω–∏–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä XLM-RoBERTa –∑ –∫–∞—Å—Ç–æ–º–Ω–æ—é
  –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ–π–Ω–æ—é –≥–æ–ª–æ–≤–æ—é.
- **–û—Å–æ–±–ª–∏–≤—ñ—Å—Ç—å:** –∑–∞–º—ñ—Å—Ç—å –æ–¥–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞ `[CLS]` –∑ –æ—Å—Ç–∞–Ω–Ω—å–æ–≥–æ —à–∞—Ä—É
  –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è *layer-wise pooling* ‚Äî —É—Å–µ—Ä–µ–¥–Ω–µ–Ω–Ω—è `[CLS]` –∑ –æ—Å—Ç–∞–Ω–Ω—ñ—Ö
  —á–æ—Ç–∏—Ä—å–æ—Ö —à–∞—Ä—ñ–≤. –¶–µ –∑–º–µ–Ω—à—É—î —à—É–º –æ–∫—Ä–µ–º–æ–≥–æ —à–∞—Ä—É —ñ —Å—Ç–∞–±—ñ–ª—ñ–∑—É—î —è–∫—ñ—Å—Ç—å.
- **–§—É–Ω–∫—Ü—ñ—è –≤—Ç—Ä–∞—Ç:** –∫—Ä–æ—Å-–µ–Ω—Ç—Ä–æ–ø—ñ—è –∑ –≤–∞–≥–∞–º–∏ –∫–ª–∞—Å—ñ–≤ (class weights),
  —â–æ –∫–æ–º–ø–µ–Ω—Å—É—î –º–æ–∂–ª–∏–≤–∏–π –¥–∏—Å–±–∞–ª–∞–Ω—Å –º—ñ–∂ GBV —Ç–∞ Non_GBV.

### –ù–∞–≤—á–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ

- **–ü–æ–∑–∏—Ç–∏–≤–Ω–∏–π –∫–ª–∞—Å (GBV):**
  - —Ç–≤—ñ—Ç–∏ –∑ –æ–∑–Ω–∞–∫–∞–º–∏ –≥–µ–Ω–¥–µ—Ä–Ω–æ –∑—É–º–æ–≤–ª–µ–Ω–æ–≥–æ –Ω–∞—Å–∏–ª—å—Å—Ç–≤–∞ (5 –ø—ñ–¥—Ç–∏–ø—ñ–≤:
    —Ñ—ñ–∑–∏—á–Ω–µ, —Å–µ–∫—Å—É–∞–ª—å–Ω–µ, –µ–º–æ—Ü—ñ–π–Ω–µ, –µ–∫–æ–Ω–æ–º—ñ—á–Ω–µ –Ω–∞—Å–∏–ª—å—Å—Ç–≤–æ, —à–∫—ñ–¥–ª–∏–≤—ñ
    —Ç—Ä–∞–¥–∏—Ü—ñ–π–Ω—ñ –ø—Ä–∞–∫—Ç–∏–∫–∏).

- **–ù–µ–≥–∞—Ç–∏–≤–Ω–∏–π –∫–ª–∞—Å (Non_GBV):**
  - —ñ–Ω—à—ñ –≤–∏–¥–∏ –º–æ–≤–∏ –Ω–µ–Ω–∞–≤–∏—Å—Ç—ñ —Ç–∞ –∞–± º—é–∑—É, –Ω–µ –ø–æ–≤ º—è–∑–∞–Ω—ñ –∑ –≥–µ–Ω–¥–µ—Ä–æ–º;
  - –∑–≤–∏—á–∞–π–Ω—ñ —Ç–≤—ñ—Ç–∏ –∑ –ø–æ–∑–∏—Ç–∏–≤–Ω–∏–º –∞–±–æ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–∏–º —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç–æ–º.

### –Ø–∫ —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç—É–≤–∞—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç

- **Non_GBV** ‚Äî –º–æ–¥–µ–ª—å –Ω–µ –±–∞—á–∏—Ç—å —á—ñ—Ç–∫–∏—Ö –æ–∑–Ω–∞–∫ –≥–µ–Ω–¥–µ—Ä–Ω–æ –∑—É–º–æ–≤–ª–µ–Ω–æ–≥–æ
  –Ω–∞—Å–∏–ª—å—Å—Ç–≤–∞; —Ç–µ–∫—Å—Ç –º–æ–∂–µ –±—É—Ç–∏ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–∏–º, –ø–æ–∑–∏—Ç–∏–≤–Ω–∏–º –∞–±–æ –º—ñ—Å—Ç–∏—Ç–∏
  —ñ–Ω—à—ñ —Ç–∏–ø–∏ –Ω–µ–≥–∞—Ç–∏–≤—É.
- **GBV** ‚Äî –º–æ–¥–µ–ª—å –≤–∏—è–≤–∏–ª–∞ –º–æ–≤–Ω—ñ –ø–∞—Ç–µ—Ä–Ω–∏, —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ñ —Å–∞–º–µ –¥–ª—è
  –≥–µ–Ω–¥–µ—Ä–Ω–æ –∑—É–º–æ–≤–ª–µ–Ω–æ–≥–æ –Ω–∞—Å–∏–ª—å—Å—Ç–≤–∞ (–æ–±—Ä–∞–∑–∏, –ø–æ–≥—Ä–æ–∑–∏, –∫–æ–Ω—Ç—Ä–æ–ª—å,
  –ø—Ä–∏–º—É—Å –¥–æ —à–∫—ñ–¥–ª–∏–≤–∏—Ö –ø—Ä–∞–∫—Ç–∏–∫ —Ç–æ—â–æ).

–ü–æ—Ä—ñ–≥ `BLOCK` –º–æ–∂–Ω–∞ –Ω–∞–ª–∞—à—Ç–æ–≤—É–≤–∞—Ç–∏. –î–ª—è –¥–æ—Å–ª—ñ–¥–Ω–∏—Ü—å–∫–∏—Ö –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ñ–≤
–¥–æ—Ü—ñ–ª—å–Ω–æ –∞–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏ —Ç–∞–∫–æ–∂ *–π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ* —Ç–∞ –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω—ñ–π
–≤–∏–±—ñ—Ä—Ü—ñ (precision, recall, F1).
        )
demo.launch()
